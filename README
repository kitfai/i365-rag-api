**To install docker**
# 1. Update package list and install dependencies
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common -y

# 2. Add Docker's official GPG key and repository
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 3. Update package list again
sudo apt update

# 4. Install Docker Engine
sudo apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y

# 5. Verify Docker installation
sudo docker --version


**To use QRANT Docker**
docker run -p 6333:6333 -p 6334:6334 -v "${PWD}/qdrant_storage:/qdrant/storage" qdrant/qdrant

**To use QDRANT as a service**
wget https://github.com/qdrant/qdrant/releases/download/v1.14.0/qdrant_1.14.0-1_amd64.deb
sudo dpkg -i qdrant_1.14.0-1_amd64.deb

sudo nano /etc/systemd/system/qdrant.service
Content of qdrant.service


[Unit]
Description=Qdrant Vector Search Engine
After=network.target

[Service]
User=qdrant
Group=qdrant
WorkingDirectory=/var/lib/qdrant
# The qdrant binary will automatically find the config file at /etc/qdrant/config.yaml
ExecStart=/usr/bin/qdrant --config-path /etc/qdrant/config.yaml
Restart=always
RestartSec=5s
# Set resource limits. Recommended for production.
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target


sudo useradd --system --user-group --shell /bin/false qdrant

# Create the data directory if it doesn't already exist
sudo mkdir -p /var/lib/qdrant

# Set the correct ownership for the data and config directories
sudo chown -R qdrant:qdrant /var/lib/qdrant
sudo chown -R qdrant:qdrant /etc/qdrant


sudo chmod -R u=rwX,g=rX,o=rX /etc/qdrant

sudo systemctl daemon-reload

sudo systemctl enable qdrant

sudo systemctl start qdrant


sudo systemctl restart qdrant

systemctl status qdrant
show see
● qdrant.service - Qdrant Vector Search Engine
     Loaded: loaded (/etc/systemd/system/qdrant.service; enabled; preset: enabled)
     Active: active (running) since Sat 2025-07-19 08:35:01 UTC; 5s ago
   Main PID: 132950 (qdrant)
      Tasks: 10 (limit: 4595)
     Memory: 25.3M
        CPU: 134ms
     CGroup: /system.slice/qdrant.service
             └─132950 /usr/bin/qdrant


To check issue on qdrant:
sudo journalctl -u qdrant -f



**Code**
sudo apt-get update && sudo apt-get install -y poppler-utils
sudo apt-get install tesseract-ocr-eng
**To Test Cuda**
import torch


print(torch.cuda.is_available())
~


sudo nano /etc/systemd/system/i365.service

[Unit]
Description=Gunicorn instance to serve i365 RAG API
After=network.target

[Service]
User=ubuntu
Group=www-data
WorkingDirectory=/home/ubuntu/git/i365-rag-api
Environment="PATH=/home/ubuntu/git/i365-rag-api/.venv/bin"
RuntimeDirectory=i365
# The umask=007 option ensures the socket is group-writable, allowing the nginx user (www-data) to connect.
# This is the fix for the '[Errno 13] Permission denied' error.
ExecStart=/home/ubuntu/git/i365-rag-api/.venv/bin/gunicorn --workers 1 --bind 'unix:/run/i365/i365.sock?umask=007' -k uvicorn.workers.UvicornWorker webapi.main:app

# Command to send SIGHUP to Gunicorn for a graceful reload
ExecReload=/bin/kill -s HUP $MAINPID

[Install]
WantedBy=multi-user.target



# This line tells systemd to create the /run/i365 directory
RuntimeDirectory=i365

ExecStart=/home/ubuntu/git/i365-rag-api/.venv/bin/gunicorn --workers 1 --bind unix:/run/i365/i365.sock -k uvicorn.workers.UvicornWorker webapi.main:app

[Install]
WantedBy=multi-user.target


systemctl daemon-reload

sudo systemctl start i365
sudo systemctl enable i365


sudo apt install nginx -y

sudo nano /etc/nginx/sites-available/i365


server {
    listen 80;
    server_name 172.16.10.7;

    location / {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_pass http://unix:/run/i365/i365.sock?umask=007;


        # --- Increase these timeout values ---
        # Default is 60s. Increase to 3 minutes (180s) or more for slow tasks.
        proxy_connect_timeout 180s;
        proxy_send_timeout    180s;
        proxy_read_timeout    180s;
        send_timeout          180s;
    }
}

sudo ln -s /etc/nginx/sites-available/i365 /etc/nginx/sites-enabled

sudo nginx -t

sudo systemctl restart nginx

sudo ufw allow 'Nginx Full'

**If there are code changes**
To reload ,
sudo systemctl reload i365

# Reload systemd to read the new ExecReload directive
sudo systemctl daemon-reload

# Restart the service to apply the new configuration
sudo systemctl restart i365

sudo systemctl reload i365
sudo systemctl status i365


When to Reload Gunicorn vs. When to Reload Nginx
This is a crucial distinction to understand for smooth deployments:

1. Reload Gunicorn
You only need to reload Gunicorn when you make changes to your Python application.

Use sudo systemctl reload i365 when:

You have pushed new Python code for your application (e.g., your FastAPI, Django, or Flask files).
You have changed a dependency in your virtual environment and re-installed packages.
2. Reload Nginx
You only need to reload Nginx when you change its own configuration files. Gunicorn changes do not require an Nginx reload.

Use sudo systemctl reload nginx when:

You have edited an Nginx configuration file (e.g., /etc/nginx/sites-available/myproject or /etc/nginx/nginx.conf).
You have added or renewed an SSL certificate (e.g., with Certbot).
Before reloading Nginx, it's always a good idea to test the configuration first:

bash
sudo nginx -t






TO check ollama is using cuda or not
journalctl -u ollama -f

should have

...
time=... level=INFO source=gpu.go:123 msg="detected NVIDIA GPU"
time=... level=INFO source=gpu.go:244 msg="total VRAM: 8192 MB, available VRAM: 8192 MB, loading model to VRAM"
...


Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context:  CUDA_Host  output buffer size =     1.19 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_kv_cache_unified:      CUDA0 KV buffer size =  1152.00 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_kv_cache_unified: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context:      CUDA0 compute buffer size =   560.00 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context:  CUDA_Host compute buffer size =    24.01 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context: graph nodes  = 1374
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context: graph splits = 2
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: time=2025-07-19T15:31:48.990Z level=INFO source=server.go:637 msg="llama runner started in 3.26 seconds"
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: [GIN] 2025/07/19 - 15:31:48 | 200 |  4.187647165s |       127.0.0.1 | POST     "/api/generate"

scale ollama

sudo systemctl edit ollama.service

add:
[Service]
Environment="OLLAMA_KEEP_ALIVE=-1"
Environment="OLLAMA_NUM_PARALLEL=2"

sudo systemctl daemon-reload
sudo systemctl restart ollama


This tells Ollama to keep the last used model in memory indefinitely.
The deepseek-r1:latest model is quite large. For a T4, especially with parallel requests, using a smaller or more heavily quantized version (like a Q4_K_M or Q5_K_M) will significantly reduce VRAM usage, prevent swapping, and improve response times.
Enable Parallel Processing with num_parallelThis directly addresses the sequential request problem. You can configure Ollama to handle multiple requests in parallel, but be very mindful of your T4's VRAM.
Add the OLLAMA_NUM_PARALLEL environment variable. A value of 2 or 3 is a reasonable starting point for a T4 GPU. Each parallel request will consume its own share of VRAM.

nvidia-smi -l 1

Monitor Real-Time VRAM UsageThis is the definitive test. You need to watch your GPU's VRAM usage as Ollama loads and runs the model. The standard tool for this is nvidia-smi.Open a terminal and run this command to get a live-updating view of your GPU status:
This command will refresh every second. Now, in another terminal, make a request to your RAG API that uses the deepseek-r1 model.What to look for in the nvidia-smi output:•Memory-Usage: This shows Used VRAM / Total VRAM. For your T4, the total will be around 16384MiB.•GPU-Util: This shows how busy the GPU's processing cores are.When you make the request, you will see the VRAM usage spike as Ollama loads the model. If the Memory-Usage gets very close to 16GB, you are at risk of performance issues or out-of-memory errors. If it comfortably sits below ~14-15GB, you are in a good spot.

journalctl -u ollama -f
Check the Ollama LogsAs you've noted in your README, the Ollama logs are a great source of information. When the model loads, it will often print details about VRAM allocation.Shell Scriptjournalctl -u ollama -fLook for lines like total VRAM: ... loading model to VRAM to confirm it's using the GPU and see how much memory it's trying to allocate.






***vllm***
    # Example for CUDA 12.1 - replace with the command for your setup
    pip install torch --index-url https://download.pytorch.org/whl/cu121


    # Uninstall both the main package and its core dependency
    pip uninstall -y outlines outlines-core

    # Re-install vllm, which will pull in the correct, compatible versions
    pip install "vllm @ git+https://github.com/vllm-project/vllm.git"


# This command starts the vllm server with a 4-bit quantized DeepSeek model

python -m vllm.entrypoints.openai.api_server \
    --model "TheBloke/deepseek-coder-1.3B-instruct-AWQ" \
    --quantization awq \
    --kv-cache-dtype fp8 \
    --gpu-memory-utilization 0.90 \
    --trust-remote-code

    to test:

    curl http://localhost:8000/v1/completions \
    -H "Content-Type: application/json" \
    -d '{
        "model": "TheBloke/deepseek-coder-1.3B-instruct-AWQ",
        "prompt": "def fibonacci(n):",
        "max_tokens": 100,
        "temperature": 0
    }'