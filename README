**To install docker**
# 1. Update package list and install dependencies
sudo apt update
sudo apt install apt-transport-https ca-certificates curl software-properties-common -y

# 2. Add Docker's official GPG key and repository
curl -fsSL https://download.docker.com/linux/ubuntu/gpg | sudo gpg --dearmor -o /usr/share/keyrings/docker-archive-keyring.gpg
echo "deb [arch=$(dpkg --print-architecture) signed-by=/usr/share/keyrings/docker-archive-keyring.gpg] https://download.docker.com/linux/ubuntu $(lsb_release -cs) stable" | sudo tee /etc/apt/sources.list.d/docker.list > /dev/null

# 3. Update package list again
sudo apt update

# 4. Install Docker Engine
sudo apt install docker-ce docker-ce-cli containerd.io docker-compose-plugin -y

# 5. Verify Docker installation
sudo docker --version


**To use QRANT Docker**
docker run -p 6333:6333 -p 6334:6334 -v "${PWD}/qdrant_storage:/qdrant/storage" qdrant/qdrant

**To use QDRANT as a service**
wget https://github.com/qdrant/qdrant/releases/download/v1.14.0/qdrant_1.14.0-1_amd64.deb
sudo dpkg -i qdrant_1.14.0-1_amd64.deb

sudo nano /etc/systemd/system/qdrant.service
Content of qdrant.service


[Unit]
Description=Qdrant Vector Search Engine
After=network.target

[Service]
User=qdrant
Group=qdrant
WorkingDirectory=/var/lib/qdrant
# The qdrant binary will automatically find the config file at /etc/qdrant/config.yaml
ExecStart=/usr/bin/qdrant --config-path /etc/qdrant/config.yaml
Restart=always
RestartSec=5s
# Set resource limits. Recommended for production.
LimitNOFILE=65535

[Install]
WantedBy=multi-user.target


sudo useradd --system --user-group --shell /bin/false qdrant

# Create the data directory if it doesn't already exist
sudo mkdir -p /var/lib/qdrant

# Set the correct ownership for the data and config directories
sudo chown -R qdrant:qdrant /var/lib/qdrant
sudo chown -R qdrant:qdrant /etc/qdrant


sudo chmod -R u=rwX,g=rX,o=rX /etc/qdrant

sudo systemctl daemon-reload

sudo systemctl enable qdrant

sudo systemctl start qdrant


sudo systemctl restart qdrant

systemctl status qdrant
show see
● qdrant.service - Qdrant Vector Search Engine
     Loaded: loaded (/etc/systemd/system/qdrant.service; enabled; preset: enabled)
     Active: active (running) since Sat 2025-07-19 08:35:01 UTC; 5s ago
   Main PID: 132950 (qdrant)
      Tasks: 10 (limit: 4595)
     Memory: 25.3M
        CPU: 134ms
     CGroup: /system.slice/qdrant.service
             └─132950 /usr/bin/qdrant


To check issue on qdrant:
sudo journalctl -u qdrant -f



**Code**
sudo apt-get update && sudo apt-get install -y poppler-utils
sudo apt-get install tesseract-ocr-eng
**To Test Cuda**
import torch


print(torch.cuda.is_available())
~


sudo nano /etc/systemd/system/i365.service

[Unit]
Description=Gunicorn instance to serve i365 RAG API
After=network.target

[Service]
User=ubuntu
Group=www-data
WorkingDirectory=/home/ubuntu/git/i365-rag-api
Environment="PATH=/home/ubuntu/git/i365-rag-api/.venv/bin"
RuntimeDirectory=i365
ExecStart=/home/ubuntu/git/i365-rag-api/.venv/bin/gunicorn --workers 1 --bind unix:/run/i365/i365.sock -k uvicorn.workers.UvicornWorker webapi.main:app

# Command to send SIGHUP to Gunicorn for a graceful reload
ExecReload=/bin/kill -s HUP $MAINPID

[Install]
WantedBy=multi-user.target



# This line tells systemd to create the /run/i365 directory
RuntimeDirectory=i365

ExecStart=/home/ubuntu/git/i365-rag-api/.venv/bin/gunicorn --workers 1 --bind unix:/run/i365/i365.sock -k uvicorn.workers.UvicornWorker webapi.main:app

[Install]
WantedBy=multi-user.targety


systemctl daemon-reload

sudo systemctl start i365
sudo systemctl enable i365


sudo apt install nginx -y

sudo nano /etc/nginx/sites-available/i365


server {
    listen 80;
    server_name 172.16.10.7;

    location / {
        proxy_set_header Host $http_host;
        proxy_set_header X-Real-IP $remote_addr;
        proxy_set_header X-Forwarded-For $proxy_add_x_forwarded_for;
        proxy_set_header X-Forwarded-Proto $scheme;
        proxy_pass http://unix:/run/i365/i365.sock;
    }
}

sudo ln -s /etc/nginx/sites-available/i365 /etc/nginx/sites-enabled

sudo nginx -t

sudo systemctl restart nginx

sudo ufw allow 'Nginx Full'

**If there are code changes**
To reload ,
sudo systemctl reload i365

# Reload systemd to read the new ExecReload directive
sudo systemctl daemon-reload

# Restart the service to apply the new configuration
sudo systemctl restart i365

sudo systemctl reload i365
sudo systemctl status i365


When to Reload Gunicorn vs. When to Reload Nginx
This is a crucial distinction to understand for smooth deployments:

1. Reload Gunicorn
You only need to reload Gunicorn when you make changes to your Python application.

Use sudo systemctl reload i365 when:

You have pushed new Python code for your application (e.g., your FastAPI, Django, or Flask files).
You have changed a dependency in your virtual environment and re-installed packages.
2. Reload Nginx
You only need to reload Nginx when you change its own configuration files. Gunicorn changes do not require an Nginx reload.

Use sudo systemctl reload nginx when:

You have edited an Nginx configuration file (e.g., /etc/nginx/sites-available/myproject or /etc/nginx/nginx.conf).
You have added or renewed an SSL certificate (e.g., with Certbot).
Before reloading Nginx, it's always a good idea to test the configuration first:

bash
sudo nginx -t






TO check ollama is using cuda or not
journalctl -u ollama -f

should have

...
time=... level=INFO source=gpu.go:123 msg="detected NVIDIA GPU"
time=... level=INFO source=gpu.go:244 msg="total VRAM: 8192 MB, available VRAM: 8192 MB, loading model to VRAM"
...


Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context:  CUDA_Host  output buffer size =     1.19 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_kv_cache_unified: kv_size = 8192, type_k = 'f16', type_v = 'f16', n_layer = 36, can_shift = 1, padding = 32
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_kv_cache_unified:      CUDA0 KV buffer size =  1152.00 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_kv_cache_unified: KV self size  = 1152.00 MiB, K (f16):  576.00 MiB, V (f16):  576.00 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context:      CUDA0 compute buffer size =   560.00 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context:  CUDA_Host compute buffer size =    24.01 MiB
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context: graph nodes  = 1374
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: llama_context: graph splits = 2
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: time=2025-07-19T15:31:48.990Z level=INFO source=server.go:637 msg="llama runner started in 3.26 seconds"
Jul 19 15:31:48 infra365-poc-aid-02-akhmw ollama[116959]: [GIN] 2025/07/19 - 15:31:48 | 200 |  4.187647165s |       127.0.0.1 | POST     "/api/generate"
